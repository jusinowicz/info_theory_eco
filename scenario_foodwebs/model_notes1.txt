#Notes 
Jan, 24, 2020; part 2: The Transfer Entropy must be considered more carefully, and my thinking on it may not have been totally correct. It is in th example here, but not in the function implemented in info_theory_functions.R. The apparent transfer entropy only considers the contribution per-source, and the nodes that can be sources must be chosen. The complete transfer entropy includes the entire set of everything that might influence the target node. But then the separable information is the sum over all of the individual apparent TEs. 
1. What is the most useful way to implement the aTE in the food web? Consider only a species' immediate interactions (i.e. a species predators, competitors, prey)? One at a time, or collectively? Each trophic level as  collection? There are multiple ways to organize this...
2. Same for the separable information: Is the sum over aTEs all members in the foodweb? Or just the same, immediate links? I guess the key is to compare a TE and SI for the same collection of potential influencers...  

Jan, 24, 2020: More exmples, trying to learn the dynamic information metrics and apply them to food web models. Today: Active Information Storage. 

Examples (from rinform vignette):

series <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
active_info(series, k = 2)

k2=k+1 
ngensa = length(series) - k+1
ngensb = length(series) - k2+1
blocks_1 = matrix(0.0, ngensb, 1) #marg of X(k)+1
blocks_k = matrix(0.0, ngensa, 1) # marg of X(k)
blocks_1k = matrix(0.0, ngensb, 1) #joint of X(k)+1, X(k) 

#Need some additional pre-formatting to make this work: 
#Find number of digits in largest integer: 
width_A = nchar(trunc(max(d1_use)))


######################
# p( X(k)+1 )
######################

for (n in 1:ngensb ) { 
	#Use width_A to format the "names" of blocks by adding zeros in front as needed
	#blocks_1[n] = paste(c (formatC(series[n],width=width_A, flag = "0" )),collapse="")
	blocks_1[n] = paste(c (formatC(series[n+k2-1],width=width_A, flag = "0" )),collapse="")
}

marg_1 =  ( prop.table(table( as.data.frame(blocks_1) )))

######################
# p( X(k) )
######################

for (n in 1:ngensa) { 
	#Use width_A to format the "names" of blocks by adding zeros in front as needed
	blocks_k[n] = paste(c (formatC(series[n:((n+k-1)) ],width=width_A, flag = "0" )),collapse="")
}

marg_k =  ( prop.table(table( as.data.frame(blocks_k) )))

######################
# p( X(k)+s,X(k) )
######################

for (n in 1:ngensb) { 
	
	#In order, this is X(k)+1, X(k)
	blocks_1k[n] =  paste(c (formatC(series[ (n+k2-1)],width=width_A, flag = "0" ), "i", formatC(series[(n:(n+k-1))],width=width_A, flag = "0" ) ),collapse="")

}

joint_1k = ( prop.table(table( as.data.frame(blocks_1k) )))

nblocks = length(joint_1k)
AIS1 =matrix(0.0, nblocks, 1)
for (n in 1:nblocks){ 

	#Split the whole block into its subblock, X(k)+1, X(k), and Y(l)
	#The right/future block
	block1 = unlist(strsplit(names(joint_1k)[n],"i") )[1] 
	#The left/past block
	block2 = unlist(strsplit(names(joint_1k)[n],"i") )[2]

	AIS1[n] = joint_1k[n]*log2( (joint_1k[n])/( marg_k[ block2 ]*marg_1[ block1 ] ) ) 
	#AIS1[n] = log2( (joint_1k[n])/( marg_k[ block2 ]*marg_1[ block1 ] ) ) 


}

sum(AIS1)


#Notes
Jan 22, 2020: More examples, trying to learn the dynamic information metrics and apply them to food web models. Today: Transfer Entropy
A useful reference: https://stats.stackexchange.com/questions/12573/calculating-the-transfer-entropy-in-r

#Will need a new kind of "block" which includes k, k(t+1) only!, and y(l), the state of #other nodes, neighbors, or entities at t. Not sure if there is a clear way to do this 
#generically for any form of y(l). So far would include: 1) One other time series, 2)
#N-other time series, 3) Nearest neighbors in space, 4)connected nodes in network. The
#network approach might be the most robust. 

#NOTE: THIS IS MASSIVELY SUPERIOR IF DONE WITH STRINGS! The advantage is only keeping track of the blocks (words) that actually appear, instead of every possible combination, whether or not it appears. 


Examples (from rinform vignette):

One initial condition, no background:

xs <- c(0, 1, 1, 1, 1, 0, 0, 0, 0)
ys <- c(0, 0, 1, 1, 1, 1, 0, 0, 0)
transfer_entropy(xs, ys, ws = NULL, k = 2)
#> [1] 0.6792696

test1 = data.frame(xs=xs,ys=ys)
ds = 2 # Which column is the series of interest? 
k=2
#Make l everything but the source series
l=(1:dim(test1)[2])[-ds]
k2=k+1 
ngensa = dim(test1)[1] - k+1
ngensb = dim(test1)[1] - k2+1
blocks_k = matrix(0.0, ngensa, 1) #Block marginals
blocks_1kl = matrix(0.0, ngensb, 1) #joint of X(k)+1, X(k), Y(l)
blocks_kl = matrix(0.0, ngensa, 1) #joint of X(k), Y(l)
blocks_1k = matrix(0.0, ngensb, 1) #joint of X(k)+1, X(k)

#Need some additional pre-formatting to make this work: 
#Find number of digits in largest integer: 
width_A = nchar(trunc(max(d1_use)))

#There are four total probabilities that are calculated here. These include three #different joint probability distributions and one marginal distribution

######################
# p( X(k) )
######################

for (n in 1:ngensa) { 
	#Use width_A to format the "names" of blocks by adding zeros in front as needed
	blocks_k[n] = paste(c (formatC(test1[n:((n+k-1)), ds ],width=width_A, flag = "0" )),collapse="")
}

marg_k =  ( prop.table(table( as.data.frame(blocks_k) )))

######################
# p( X(k)+1,X(k),Y(l) )
######################

for (n in 1:ngensb) { 
	
	#In order, this is X(k)+1, X(k),Y(l)
	blocks_1kl[n] =  paste(c (formatC(test1[n+k2-1,ds],width=width_A, flag = "0" ), "i", formatC(test1[(n:(n+k-1)),ds],width=width_A, flag = "0" ),"i",
	formatC(test1[(n+k-1),l],width=width_A, flag = "0" )  ),collapse="")

}

joint_1kl =   ( prop.table(table( as.data.frame(blocks_1kl) )))

######################
# p( X(k),Y(l) )
######################

for (n in 1:ngensa) { 
	
	#In order, this is X(k),Y(l)
	blocks_kl[n] =  paste(c (formatC(test1[(n:(n+k-1)),ds],width=width_A, flag = "0" ),"i", formatC(test1[(n+k-1),l],width=width_A, flag = "0" )  ),collapse="")

}
joint_kl =  ( prop.table(table( as.data.frame(blocks_kl) )))

######################
# p( X(k)+s,X(k) )
######################

for (n in 1:ngensb) { 
	
	#In order, this is X(k)+1, X(k)
	blocks_1k[n] =  paste(c (formatC(test1[ (n+k2-1),ds],width=width_A, flag = "0" ), "i", formatC(test1[(n:(n+k-1)),ds],width=width_A, flag = "0" ) ),collapse="")

}

joint_1k = ( prop.table(table( as.data.frame(blocks_1k) )))

nblocks = length(joint_1kl)
TE1 =matrix(0.0, nblocks, 1)
for (n in 1:nblocks){ 

	#Split the whole block into its subblock, X(k)+1, X(k), and Y(l)
	#The right/future block
	block1 = unlist(strsplit(names(joint_1kl)[n],"i") )[1] 
	#The left/past block
	block2 = unlist(strsplit(names(joint_1kl)[n],"i") )[2]
	#The neighborhood block
	block3 = unlist(strsplit(names(joint_1kl)[n],"i") )[3]

	TE1[n] = joint_1kl[n]*log2( (joint_1kl[n]*marg_k[block2])/( joint_kl[ paste( c(block2,"i",block3) ,collapse="") ]*joint_1k[paste( c(block1,"i",block2) ,collapse="") ] ) ) 

}

sum(TE1)

#My numbers are always slightly different than the rinform numbers, but close. But 
#they consistently match up with other examples. Like the one at 
#http://users.utu.fi/attenka/TEpresentation081128.pdf

TPvector2T = marg_k
TPvector3T = joint_kl
TPvector4T = joint_1k
        SUMvector[n]=TPvector1T[n]*log10((TPvector1T[n]*TPvector2T[(unlist(strsplit(names(TPvector1T)[n],"i")))[2]])/(TPvector3T[paste((unlist(strsplit(names(TPvector1T)[n],"i")))[2],"i",(unlist(strsplit(names(TPvector1T)[n],"i")))[3],sep="",collapse="")]*TPvector4T[paste((unlist(strsplit(names(TPvector1T)[n],"i")))[1],"i",(unlist(strsplit(names(TPvector1T)[n],"i")))[2],sep="",collapse="")]))

###


Jan 8-20, 2020: Another example, trying to figure out how to apply the dynamic information metrics to the food web models. (Univariate) Excess Entropy

library(rinform)

#For comparison, do this all from scratch using my functions for the Rutledge version
#of MI
#Define shannon_D in base 2
shannon_D2 = function ( freq = freq) {

	sD =  - sum ( freq*log2(freq),na.rm=T )
	return(sD)

}

#Define the MI in base 2
get_mIb2 = function (fij=fij) {
	ncol1 = dim(fij)[1]

	fij_x = rowSums(fij) #Marginals of x and y
	fij_y = colSums(fij)
	
	mI = NULL

	pxpy=t(t(fij_x))%*%fij_y #This is p(y)*p(x)
	itmp=(fij*log2(fij/pxpy)); itmp[is.na(itmp)]=0
	mI$mean = sum(rowSums(itmp, na.rm=T),na.rm=T) #Mutual information
	mI$per = itmp 

	return(mI)
}

#Use this function to simulate the birth-death process
get_bdi_lgs2 = function (bs, ds, gs, Ki, nsims=30000, ip=1) {

	out=NULL
	pop = matrix(0,nsims,1) #Population matrix (queue)
	gr = matrix(0,nsims,1) #Pop growth rate
	probs = matrix(0,nsims,1) #Each transition probability
	pop[1] = ip
	p = runif(nsims,0,1) #Probabilities of birth/death
	pt =1 

	for (t in 2:nsims) { 
 		if(pt>0) {
 			pop[t] = pt
 			#Make bs ands ds density-dependent
 			bs_d = bs*pop[t]+gs
			ds_d = ds*pop[t]^2
			gr[t] = bs_d-ds_d
			probs[t] = bs_d/(bs_d+ds_d)
			pt=ifelse (p[t]<bs_d/(bs_d+ds_d),
	            pt+1, # arrival
	            pt-1) # departure
		} else {
			pop[t] = pt
			if(p[t]<bs/(bs+ds)){pt=1}
	        gr[t] = bs-ds 
	        probs[t] = bs/(bs+ds) 

		}
	}
	out$pop = pop
	out$probs = probs
	out$gr = gr
	return(out)
}


#Birth rate
r = 1.1
#Carrying capacity
K=30
#Mean death rate
ds = r/K
#Mean immigration rate
gs = 0

pop_bdi_d = get_bdi_lgs2(r,ds,gs,K)
par(mfrow=c(2,1))
d1 = data.frame (pop=pop_bdi_d$pop, gr=pop_bdi_d$gr)
plot(d1$pop,d1$gr)
abline(h=0)
abline(v=K)
d2 = data.frame (pop=pop_bdi_d$pop, probs=pop_bdi_d$probs)
plot(d2$pop,d2$probs)
abline(h=0.5)
abline(v=K)

#Calculate the k-block excess entropy using rinform library: 
d1_use = d1$pop[100:200]
d1_use = d1_use - min(d1_use) #Something about the rinform code, it works better this way
k = 2
excess_entropy(c(d1_use),k=k)

#Generate each of k-blocks for the time series
#Find the marginals of each k block
k2=k*2 
ngensb = length(d1_use) - k2-1
blocks_k2 = matrix(0.0, ngensb, k2) #joint
blocks_k = matrix(0.0, ngensb, k) #Marginals
#l1rb = matrix(0.0, ngensb, k2)
for (n in 1:ngensb) { 
	blocks_k[n,] = d1_use[n:((n+k-1))]
	blocks_k2[n,] =  d1_use[n:((n+k2-1))]
	#l1rb[n,] = l1r[n:((n+k-1))]
}

#Prob of each symbol in each vector
X_probs = apply( (apply(blocks_k2,2,table)),2,  prop.table) 
#Marginal probabilities of blocks k
#The length of X_marg tell us our new alphabet
X_marg = ( prop.table(table( as.data.frame(blocks_k) )))
#Joint probabilities 
X_joint = ( prop.table(table( as.data.frame(blocks_k2) )))
#Joint probabilities across all
#X_joint = c( prop.table(table( as.data.frame(blocks_k2) ))) 


#Turn these into a 1D vector of marginals and a 2D matrix of joint probabilities
#The first k indexes in the matrix X_joint are the block. 
X_marg_c = c(X_marg)
new_alph = length(X_marg)
X_joint_c = matrix(X_joint, new_alph,new_alph)
#E.g. 18,19 is X_marg[1,2] and 20,19 is X_marg[3,2] and these are in positions 13 and 15
#in X_marg_c. These 4 indexes correspond to X_joint[1,2,3,2]. 
#The matrix resizing of X_joint takes each column of each "page" and stacks them into #the corresponding column of X_joint_c. I.e. X_joint[,,1,1] becomes X_joint_c[,1].
# 

mi2 = get_mIb2(X_joint_c)
mi2$mean

21,22,23,24,25,26
0.33000000 0.29333333 0.25666667 0.22000000  0.18333333

#How are these empirical probabilities related to the ones calculated with knowledge
#of the population process? Can a k block be reconstructed by multiplying underlying 
#probabilities? E.g.
21->22->23->24->25->26
0.33000000 0.29333333 0.25666667 0.22000000  0.18333333
#It's close, but also need the probability of finding the process in state X,i.e. the 
#the stationary state here. 


####
#Alternatively, do this with text. Compare speed/memory concerns. 
#Generate each of k-blocks for the time series
#Find the marginals of each k block
k2=k*2 
ngensa = length(d1_use) - k+1
ngensb = length(d1_use) - k2+1
blocks_k2 = matrix(0.0, ngensb, 1) #joint
blocks_k = matrix(0.0, ngensb, 1) #Marginals

#Need some additional pre-formatting to make this work: 
#Find number of digits in largest integer: 
width_A = nchar(trunc(max(d1_use)))

for (n in 1:ngensa) { 
	#Use width_A to format the "names" of blocks by adding zeros in front as needed
	blocks_k[n] = paste(c (formatC(d1_use[n:((n+k-1))],width=width_A, flag = "0" )),collapse="")
}

for (n in 1:ngensb) { 
	
	blocks_k2[n] =  paste(c (formatC(d1_use[n:((n+k-1))],width=width_A, flag = "0" ), "i", formatC(d1_use[((n+k):(n+k2-1))],width=width_A, flag = "0" ) ),collapse="")

}

#Prob of each symbol in each vector
X_probs = apply( (apply(blocks_k2,2,table)),2,  prop.table) 
#Marginal probabilities of blocks k
#The length of X_marg tell us our new alphabet
X_marg = ( prop.table(table( as.data.frame(blocks_k) )))
#Joint probabilities 
X_joint = ( prop.table(table( as.data.frame(blocks_k2) )))
#Joint probabilities across all
#X_joint = c( prop.table(table( as.data.frame(blocks_k2) ))) 

nblocks = length(X_joint)
MI1 =matrix(0.0, nblocks, 1)
for (n in 1:nblocks){ 
	#The left/past block
	block1 = unlist(strsplit(names(X_joint)[n],"i") )[1] 
	#The right/future block
	block2 = unlist(strsplit(names(X_joint)[n],"i") )[2]
	MI1[n] = X_joint[n]*log2(X_joint[n]/( X_marg[block1]*X_marg[block2]  )) 
}

###



December 13, 2019: The way forward is to think about information processing specifically, and try to mimic some of the information-theoretic approaches that have been applied elsewhere -- notably, in neuroscience. This is the lowest hanging fruit, in terms of asking whether foodwebs display information processing capabilities, and identifying tools that could quickly be ported to exiting data sets. 

In this regard, start using the R library "rinform" and it's accompanying documentation to understand the dynamic information theoretic measures that can help elucidate generic information processing features. 

For example, trying to understand the multivariate indexing over multiple variables, vs. the local indexing of a single process. Start with the Multivariate Mutual Information (e.g. Eqn. A7 in the JDITdoc.pdf manual). From the rinform vignette, Multivariate Mutual Information:

Examples:

Black-box 4 time series (each of length 8) into a single time series:

xs      <- matrix(0, nrow = 8, ncol = 4)
xs[, 1] <- c(0, 1, 1, 0, 1, 0, 0, 1)
xs[, 2] <- c(1, 0, 0, 1, 1, 0, 1, 0)
xs[, 3] <- c(0, 0, 0, 1, 1, 1, 0, 0)
xs[, 4] <- c(1, 0, 1, 0, 1, 1, 1, 0)
parts   <- c(1, 1, 1, 1)
black_box_parts(xs, parts)
This could have been more simple using black_box, but it is for illustrative purpose.

Black-box 4 time series (of length 8) into two time series using the partitioning scheme $(1,2,2,1)$. That is, combine the 1st and 4th, and the 2nd and 3rd.

parts   <- c(1, 2, 2, 1)
x <- black_box_parts(xs, parts)
x
Note that the two time series each have a base of 4, and the bases are returned as $b.

Finally, in this example, we compute the multivariate mutual information between 4 time series (each of length 8) for all partitionings of the system (except the very first since that only yields a single time series). That is, each partition is treated as a seperate variable and the mutual information is computed between the partitioned variables.

xs         <- matrix(0, nrow = 8, ncol = 4)
xs[, 1]    <- c(0, 1, 1, 0, 1, 0, 0, 1)
xs[, 2]    <- c(1, 0, 0, 1, 1, 0, 1, 0)
xs[, 3]    <- c(0, 0, 0, 1, 1, 1, 0, 0)
xs[, 4]    <- c(1, 0, 1, 0, 1, 1, 1, 0)
all_parts  <- partitioning(4)
mmi        <- rep(0, dim(all_parts)[2] - 1)

for (i in 2:dim(all_parts)[2]) {
  x          <- black_box_parts(xs, all_parts[, i])
  mmi[i - 1] <- mutual_info(x$box)
}
round(mmi, 3)

The calculation of the mmi between the 4 time series is equivalent to: 

sum (H ( each series )) - H(X1,X2,X3,X4)

In R: 

X_probs = apply( (apply(xs,2,table)),2,  prop.table) #Prob of 0 and 1 in each vector
X_joint = c( prop.table(table( as.data.frame(xs) ))) #Joint probabilities across all 4 vectors
mmi4 =sum ( apply(X_probs,2,shannon_D2) ) - shannon_D2(X_joint)


October 18, 2019: There are a few different ways to build the compartment model/web out of the underlying functions. Obviously, the fact that this specification is not unique is somewhat troubling. But I think the only thing to do at this point is to be able to enumerate the main ways that the specification can change and be able to account for the impacts of these changes on the information theoretic quantities. 
	1.	With (1+nRsp+2*nCsp+2*nPsp+1)^2 compartments. The first row/column represents 
		biomass/energetic input into the system. The last row/column represents mortality and thus biomass lost from the system. The inefficiency in consumption -> biomass production is accounted for. 
	2.	With (nRsp+2*nCsp+2*nPsp+1)^2 compartments. The last row/column represents mortality	and thus biomass lost from the system. The inefficiency in consumption -> biomass 		production is accounted for. 
	3.	With (nRsp+2*nCsp+2*nPsp+1)^2 compartments. The last row/column represents mortality	and thus biomass lost from the system. The inefficiency in consumption -> biomass 		production is accounted for. However, the resource biomass is assumed to stay in the 	 system instead of going into the mortality bin --> EDIT: This shouldn't make a 			difference because only the location of the compartments changes, but the number of 	compartments and strength of the links stays the same.
	4.  With (nRsp+nCsp+nPsp+1)^2 compartments. The last row/column represents mortality		and thus biomass lost from the system. There is no accounting for inefficiency. Does	this throw off the overall balance of the system? Look at this later.   

October 17, 2019: Here is a key point that I was missing: The fij are actually conditional probabilities, and NOT joint probabilities. This is highlighted in eqn. (5) and eqn (6) of Rutledge et al 1976. This makes a big difference to the math. Assuming that the fij are conditional probabilities, then it actually takes a little effort to extract the joint probabilities. From the definition of conditional and joint probabilities (using the notation of eqn 5 and 6) P(ak/bk)/P(ak) = P(ak,bk)/(P(ak)*P(bk)) because P(ak/bk) = P(ak,bk)/P(bk).  


October 3, 2019: The food web model becomes novel when we add compartments for wasted energy. An innovation that might turn out to be key is the fact that "waste" is essentially defined by the amount of biomass that is consumed by, but not used by the next trophic level up. This has the effect of scaling the biomass consumed into an "edible biomass." In an emprical setting, what we would want to know is the absolute biomass of an organism relative to that which has nutritional value -- presumably, the non-nutritional biomass is almost entirely related to structures etc that make the organism inedible, and so are themselves inedible. 